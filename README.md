We first attempted to filter out open-source software (OSS) projects that had not been active for a long time from a pool of 6,000 OSS projects. Using the GitHub API endpoint, we determined the duration distribution and plotted four types of graphs for these projects: (i) all projects without filters, (ii) with a 30-day filter, (iii) with a 60-day filter, and (iv) with a 90-day filter. However, we ultimately abandoned this approach due to survivorship bias and the surge in cryptocurrency activity in 2020.
Next, we analyzed 600 projects using ChatGPT for two distinct AI scenarios: one with a framework providing specific classification key terms for ChatGPT to select from, and another without the framework, where ChatGPT generated classifications independently before sorting them into given categories. We compared the classification results from both scenarios with manually collected classifications of over 600 OSS projects from the summer. The framework-based scenario resulted in a slightly lower mismatch rate with the manually collected classifications. Specifically, the Funding model mismatch rate was 73%, the Governance model mismatch rate was 95%, and the Project type mismatch rate was 69%.
To ensure the accuracy of the manually collected classifications, we double-checked 60 projects, 46 of which had clear classifications. We created a confusion matrix (Figure 1), where the y-axis represents the actual classifications and the x-axis represents the AI classifications. The diagonal entries show projects that the AI classified correctly. The AI correctly classified 50% of the projects, with the highest number of accurate classifications in the "Product/Service Sales Income" category. Most classifications clustered around the diagonal.
We then used ChatGPT-4 to classify the funding models of 516 manually collected OSS projects. These projects were submitted to ChatGPT in batches of 12, with project names and GitHub links provided for classification. Clear instructions directed ChatGPT to review each project’s GitHub page, followed by its whitepaper, CoinMarketCap profile, and official website if available. The instructions also included a list of funding model categories for ChatGPT to choose from. However, after processing 3–4 batches, ChatGPT tended to classify all projects under an "unspecified" category. We reiterated the instructions and classification steps, which improved accuracy upon reclassification. Weighted dual classifications were used in the confusion matrix, as certain projects were classified under multiple funding models. The AI classification accuracy compared to manually collected funding models was 47.4% (Figure 2).
To address mismatches, particularly between "Donations" and "Crowdfunding Without Token," we enhanced ChatGPT-4 prompts by including definitions for both categories. We manually messaged ChatGPT with batches of six projects to improve classification accuracy and avoid unsupported assumptions. Each message included the following prompt:
“First, review each project's GitHub repository, white paper, CoinMarketCap, and official website (if available). Based on this information and documentation, classify each project's funding model (choose from the options below and list all applicable primary funding models): Public Token Sale, Crowdfunding Without Token, Product/Service Sales Income, Donations, and Others. The definition of 'Donations' is a gift of money, goods, services, or time made to an organization by an individual or entity without expecting anything in return. The definition of 'Crowdfunding Without Token' is funding a project or venture by raising money from a large number of people, typically via the internet. Carefully distinguish between these two funding models. Please list all applicable funding models, as more than one may apply.”
Even with improved prompts, ChatGPT tended to overlook instructions after 3–4 batches. When discrepancies arose between AI-classified and manually collected funding models, we queried ChatGPT about specific projects to verify if the manually collected funding model could also be a primary model.
After processing all 516 projects, we created a weighted confusion matrix to capture overlaps between AI-classified and actual funding models (Figure 3). This approach improved the accuracy rate to 49%. Additionally, we created another confusion matrix (Figure 4), where overlapping models replaced classifications. This further increased accuracy to 53.1%.
To enhance accuracy further, we updated the prompt by removing the "Others" label. The revised prompt is:
“First, review each project's GitHub repository, white paper, CoinMarketCap, and official website (if available). Based on this information and documentation, classify each project's funding model (choose from the options below and list all applicable primary funding models): Public Token Sale, Crowdfunding Without Token, Product/Service Sales Income, Donations. The definition of 'Public Token Sale' is the sale of blockchain-based tokens directly to the public, typically through Initial Coin Offerings (ICOs), in exchange for cryptocurrency or fiat money. The definition of 'Product/Service Sales Income' is revenue generated by selling goods or services directly related to the project, such as subscription fees, transaction fees, or applications. The definition of 'Donations' is a gift of money, goods, services, or time made to an organization by an individual or entity without expecting anything in return. The definition of 'Crowdfunding Without Token' is funding a project or venture by raising money from a large number of people, typically via the internet. Carefully distinguish between these funding models. Please list all applicable funding models (more than one may apply). Review the following projects.”
We analyzed 45 projects individually using this prompt. To compute similarity between actual and AI-classified funding models, we represented them as binary vectors, with each category (e.g., Public Token Sale, Product/Service Sales Income) coded as 1 (present) or 0 (absent). Using Manhattan Distance to measure differences, we achieved a similarity rate of 70.6% for these projects. This marked a significant improvement, demonstrating the efficacy of refined prompts and methodologies. Refer to Figure 5 for the updated confusion matrix and results.
We applied the same AI model to 200 projects, processing 3-4 projects per batch by manually sending prompts to ChatGPT, achieving an accuracy rate of 69.12%. Subsequently, we applied the same model to 1,000 projects, processing 5 projects per batch using the ChatGPT 4.0 API manually. For this, we followed the protocol: Response 1/Response 2 – run several requests until data from the two versions converged. In case of other errors, a new chat was initiated. Additionally, we developed an API script to automate ChatGPT queries. However, the newest model available via the API was ChatGPT 3.5 Turbo. To address this, we used Selenium with a Google Chrome driver to create an automated chat. Please reference the following files for details: Funding_models_classifications – for API-based automation using GPT 3.5; Funding_models_classifications_Selenium – for automation using Selenium.
